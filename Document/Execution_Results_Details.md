# Execution Results with Different Specification

<details>
<summary>1. LeNet-5 - Original - MNIST</summary>

    ../data/mnist/
    mnist train number: 60000
    mnist test number: 10000
    weight: 25 x 6
    weight: 150 x 16
    0-th batch, loss: 2.30106
    50-th batch, loss: 2.30161
    100-th batch, loss: 2.30255
    150-th batch, loss: 2.30216
    200-th batch, loss: 2.30364
    250-th batch, loss: 2.30169
    300-th batch, loss: 2.29974
    350-th batch, loss: 2.3047
    400-th batch, loss: 2.29766
    450-th batch, loss: 2.30504

    1-th epoch, test acc: 0.1135

    0-th batch, loss: 2.29936
    50-th batch, loss: 2.30154
    100-th batch, loss: 2.29908
    150-th batch, loss: 2.30656
    200-th batch, loss: 2.29897
    250-th batch, loss: 2.29905
    300-th batch, loss: 2.29567
    350-th batch, loss: 2.29643
    400-th batch, loss: 2.29737
    450-th batch, loss: 2.29367

    2-th epoch, test acc: 0.2945

    0-th batch, loss: 2.29268
    50-th batch, loss: 2.25947
    100-th batch, loss: 1.4935
    150-th batch, loss: 0.656153
    200-th batch, loss: 0.484903
    250-th batch, loss: 0.371444
    300-th batch, loss: 0.394418
    350-th batch, loss: 0.344895
    400-th batch, loss: 0.147748
    450-th batch, loss: 0.207219

    3-th epoch, test acc: 0.946

    0-th batch, loss: 0.309088
    50-th batch, loss: 0.155788
    100-th batch, loss: 0.126408
    150-th batch, loss: 0.0778514
    200-th batch, loss: 0.0862488
    250-th batch, loss: 0.109127
    300-th batch, loss: 0.187663
    350-th batch, loss: 0.307188
    400-th batch, loss: 0.0749426
    450-th batch, loss: 0.233346

    4-th epoch, test acc: 0.9712

    0-th batch, loss: 0.106017
    50-th batch, loss: 0.225058
    100-th batch, loss: 0.1098
    150-th batch, loss: 0.0508292
    200-th batch, loss: 0.161406
    250-th batch, loss: 0.0563152
    300-th batch, loss: 0.0594433
    350-th batch, loss: 0.0520018
    400-th batch, loss: 0.0731089
    450-th batch, loss: 0.13004

    5-th epoch, test acc: 0.9737

</details>

<details>
<summary>2. LeNet-5 - Original - Fashion-MNIST</summary>

    ../data/fashion-mnist/
    mnist train number: 60000
    mnist test number: 10000
    weight: 25 x 6
    weight: 150 x 16
    0-th batch, loss: 2.30187
    50-th batch, loss: 2.30262
    100-th batch, loss: 2.30198
    150-th batch, loss: 2.30255
    200-th batch, loss: 2.30246
    250-th batch, loss: 2.30161
    300-th batch, loss: 2.30366
    350-th batch, loss: 2.30217
    400-th batch, loss: 2.30177
    450-th batch, loss: 2.30305

    1-th epoch, test acc: 0.1

    0-th batch, loss: 2.30196
    50-th batch, loss: 2.30234
    100-th batch, loss: 2.302
    150-th batch, loss: 2.30173
    200-th batch, loss: 2.29918
    250-th batch, loss: 2.2945
    300-th batch, loss: 2.25686
    350-th batch, loss: 1.64056
    400-th batch, loss: 1.09968
    450-th batch, loss: 0.895271

    2-th epoch, test acc: 0.6223

    0-th batch, loss: 0.971867
    50-th batch, loss: 0.85752
    100-th batch, loss: 0.882794
    150-th batch, loss: 0.819006
    200-th batch, loss: 0.613208
    250-th batch, loss: 0.582175
    300-th batch, loss: 0.819144
    350-th batch, loss: 0.564239
    400-th batch, loss: 0.732092
    450-th batch, loss: 0.458242

    3-th epoch, test acc: 0.7613

    0-th batch, loss: 0.630592
    50-th batch, loss: 0.576978
    100-th batch, loss: 0.603608
    150-th batch, loss: 0.73861
    200-th batch, loss: 0.670806
    250-th batch, loss: 0.409765
    300-th batch, loss: 0.630083
    350-th batch, loss: 0.487741
    400-th batch, loss: 0.57684
    450-th batch, loss: 0.591467

    4-th epoch, test acc: 0.7909

    0-th batch, loss: 0.550854
    50-th batch, loss: 0.46269
    100-th batch, loss: 0.406975
    150-th batch, loss: 0.548449
    200-th batch, loss: 0.416689
    250-th batch, loss: 0.460913
    300-th batch, loss: 0.708948
    350-th batch, loss: 0.484831
    400-th batch, loss: 0.531239
    450-th batch, loss: 0.476569

    5-th epoch, test acc: 0.8207

</details>

<details>
<summary>3. LeNet-5 - Original - Transpose Matrix Multiplication - Fashion-MNIST</summary>

    0-th batch, loss: 2.30187
    50-th batch, loss: 2.30262
    100-th batch, loss: 2.30198
    150-th batch, loss: 2.30255
    200-th batch, loss: 2.30246
    250-th batch, loss: 2.30161
    300-th batch, loss: 2.30366
    350-th batch, loss: 2.30217
    400-th batch, loss: 2.30177
    450-th batch, loss: 2.30305

    1-th epoch, test acc: 0.1

    0-th batch, loss: 2.30196
    50-th batch, loss: 2.30234
    100-th batch, loss: 2.302
    150-th batch, loss: 2.30173
    200-th batch, loss: 2.29919
    250-th batch, loss: 2.29452
    300-th batch, loss: 2.25696
    350-th batch, loss: 1.64363
    400-th batch, loss: 1.10052
    450-th batch, loss: 0.890389

    2-th epoch, test acc: 0.6231

    0-th batch, loss: 0.982203
    50-th batch, loss: 0.8619
    100-th batch, loss: 0.883574
    150-th batch, loss: 0.814321
    200-th batch, loss: 0.615839
    250-th batch, loss: 0.580562
    300-th batch, loss: 0.838665
    350-th batch, loss: 0.565908
    400-th batch, loss: 0.728824
    450-th batch, loss: 0.458302

    3-th epoch, test acc: 0.7602

    0-th batch, loss: 0.633762
    50-th batch, loss: 0.577861
    100-th batch, loss: 0.612528
    150-th batch, loss: 0.725019
    200-th batch, loss: 0.652117
    250-th batch, loss: 0.411808
    300-th batch, loss: 0.626963
    350-th batch, loss: 0.49442
    400-th batch, loss: 0.56591
    450-th batch, loss: 0.585407

    4-th epoch, test acc: 0.7898

    0-th batch, loss: 0.556282
    50-th batch, loss: 0.47116
    100-th batch, loss: 0.391514
    150-th batch, loss: 0.543599
    200-th batch, loss: 0.431745
    250-th batch, loss: 0.475138
    300-th batch, loss: 0.696249
    350-th batch, loss: 0.491586
    400-th batch, loss: 0.538841
    450-th batch, loss: 0.493691

    5-th epoch, test acc: 0.8194

</details>

<details>
<summary>4. Lenet-5 - Sequential (Im2Col) - <s>Transpose Matrix Multiplication</s></summary>

    ../data/fashion-mnist/
    mnist train number: 60000
    mnist test number: 10000
    0-th batch, loss: 2.30436, time forwading avearage: 948.853
    50-th batch, loss: 2.30307, time forwading avearage: 944.659
    100-th batch, loss: 2.30421, time forwading avearage: 933.365
    150-th batch, loss: 2.30438, time forwading avearage: 925.608
    200-th batch, loss: 2.30265, time forwading avearage: 922.035
    250-th batch, loss: 2.30305, time forwading avearage: 919.929
    300-th batch, loss: 2.30205, time forwading avearage: 917.199
    350-th batch, loss: 2.30245, time forwading avearage: 918.482
    400-th batch, loss: 2.3033, time forwading avearage: 917.907
    450-th batch, loss: 2.30154, time forwading avearage: 916.248

    1-th epoch, test acc: 0.1

    0-th batch, loss: 2.30235, time forwading avearage: 916.03
    50-th batch, loss: 2.3019, time forwading avearage: 916.789
    100-th batch, loss: 2.30197, time forwading avearage: 917.071
    150-th batch, loss: 2.30126, time forwading avearage: 916.358
    200-th batch, loss: 2.29878, time forwading avearage: 915.882
    250-th batch, loss: 2.29522, time forwading avearage: 916.141
    300-th batch, loss: 2.19485, time forwading avearage: 916.126
    350-th batch, loss: 1.52532, time forwading avearage: 915.552
    400-th batch, loss: 1.03688, time forwading avearage: 915.42
    450-th batch, loss: 0.98587, time forwading avearage: 915.087

    2-th epoch, test acc: 0.6093

    0-th batch, loss: 0.816876, time forwading avearage: 915.318
    50-th batch, loss: 0.873442, time forwading avearage: 914.757
    100-th batch, loss: 0.637464, time forwading avearage: 914.114
    150-th batch, loss: 0.89731, time forwading avearage: 913.469
    200-th batch, loss: 0.781668, time forwading avearage: 913.548
    250-th batch, loss: 0.702516, time forwading avearage: 913.086
    300-th batch, loss: 0.634439, time forwading avearage: 912.829
    350-th batch, loss: 0.765963, time forwading avearage: 912.385
    400-th batch, loss: 0.565993, time forwading avearage: 912.123
    450-th batch, loss: 0.692901, time forwading avearage: 912.169

    3-th epoch, test acc: 0.7663

    0-th batch, loss: 0.474801, time forwading avearage: 911.686
    50-th batch, loss: 0.562753, time forwading avearage: 911.3
    100-th batch, loss: 0.503141, time forwading avearage: 911.083
    150-th batch, loss: 0.501888, time forwading avearage: 911.479
    200-th batch, loss: 0.60367, time forwading avearage: 911.21
    250-th batch, loss: 0.631846, time forwading avearage: 911.003
    300-th batch, loss: 0.439007, time forwading avearage: 910.801
    350-th batch, loss: 0.581682, time forwading avearage: 910.625
    400-th batch, loss: 0.477746, time forwading avearage: 910.687
    450-th batch, loss: 0.382991, time forwading avearage: 910.305

    4-th epoch, test acc: 0.8123

    0-th batch, loss: 0.456361, time forwading avearage: 910.117
    50-th batch, loss: 0.394907, time forwading avearage: 909.963
    100-th batch, loss: 0.622893, time forwading avearage: 909.893
    150-th batch, loss: 0.633183, time forwading avearage: 909.989
    200-th batch, loss: 0.413122, time forwading avearage: 909.932
    250-th batch, loss: 0.415152, time forwading avearage: 909.816
    300-th batch, loss: 0.497327, time forwading avearage: 909.715
    350-th batch, loss: 0.581514, time forwading avearage: 909.542
    400-th batch, loss: 0.40746, time forwading avearage: 909.575
    450-th batch, loss: 0.429998, time forwading avearage: 909.324

    5-th epoch, test acc: 0.8297

</details>

<details>
<summary>5.1. Lenet-5 - Parallel Version 1 (Im2Col) - Get unrolled image matrix by Map (Eigen)</s></summary>

    ../data/fashion-mnist/
    mnist train number: 60000
    mnist test number: 10000
    0-th batch, loss: 2.30443, time forwading avearage: 519.242
    50-th batch, loss: 2.30312, time forwading avearage: 558.582
    100-th batch, loss: 2.30426, time forwading avearage: 554.799
    150-th batch, loss: 2.30444, time forwading avearage: 552.319
    200-th batch, loss: 2.30273, time forwading avearage: 553.579
    250-th batch, loss: 2.30315, time forwading avearage: 552.759
    300-th batch, loss: 2.30216, time forwading avearage: 553.246
    350-th batch, loss: 2.30258, time forwading avearage: 554.074
    400-th batch, loss: 2.30353, time forwading avearage: 552.89
    450-th batch, loss: 2.30178, time forwading avearage: 552.752

    1-th epoch, test acc: 0.1

    0-th batch, loss: 2.30272, time forwading avearage: 552.888
    50-th batch, loss: 2.30233, time forwading avearage: 552.339
    100-th batch, loss: 2.30278, time forwading avearage: 551.481
    150-th batch, loss: 2.30267, time forwading avearage: 551.202
    200-th batch, loss: 2.30221, time forwading avearage: 550.748
    250-th batch, loss: 2.3022, time forwading avearage: 551.043
    300-th batch, loss: 2.30265, time forwading avearage: 550.898
    350-th batch, loss: 2.30239, time forwading avearage: 550.99
    400-th batch, loss: 2.30229, time forwading avearage: 550.944
    450-th batch, loss: 2.30158, time forwading avearage: 550.741

    2-th epoch, test acc: 0.1

    0-th batch, loss: 2.30225, time forwading avearage: 550.656
    50-th batch, loss: 2.30217, time forwading avearage: 550.4
    100-th batch, loss: 2.30166, time forwading avearage: 550.595
    150-th batch, loss: 2.30137, time forwading avearage: 550.681
    200-th batch, loss: 2.3002, time forwading avearage: 550.212
    250-th batch, loss: 2.29857, time forwading avearage: 550.816
    300-th batch, loss: 2.28584, time forwading avearage: 550.9
    350-th batch, loss: 2.19746, time forwading avearage: 551.888
    400-th batch, loss: 2.10948, time forwading avearage: 552.429
    450-th batch, loss: 2.08172, time forwading avearage: 553.207

    3-th epoch, test acc: 0.2014

    0-th batch, loss: 2.08492, time forwading avearage: 553.253
    50-th batch, loss: 1.97053, time forwading avearage: 553.886
    100-th batch, loss: 1.35175, time forwading avearage: 554.103
    150-th batch, loss: 1.21533, time forwading avearage: 554.42
    200-th batch, loss: 1.15011, time forwading avearage: 554.305
    250-th batch, loss: 1.12398, time forwading avearage: 553.986
    300-th batch, loss: 0.979736, time forwading avearage: 553.293
    350-th batch, loss: 1.00885, time forwading avearage: 552.745
    400-th batch, loss: 0.991454, time forwading avearage: 552.427
    450-th batch, loss: 0.776683, time forwading avearage: 551.959

    4-th epoch, test acc: 0.6465

    0-th batch, loss: 0.843277, time forwading avearage: 551.73
    50-th batch, loss: 0.817378, time forwading avearage: 551.371
    100-th batch, loss: 0.853431, time forwading avearage: 551.12
    150-th batch, loss: 1.0095, time forwading avearage: 550.743
    200-th batch, loss: 0.78268, time forwading avearage: 550.508
    250-th batch, loss: 0.783464, time forwading avearage: 550.298
    300-th batch, loss: 0.730238, time forwading avearage: 550.427
    350-th batch, loss: 0.931188, time forwading avearage: 550.089
    400-th batch, loss: 0.778117, time forwading avearage: 550.036
    450-th batch, loss: 0.733352, time forwading avearage: 549.969

    5-th epoch, test acc: 0.7081

</details>

<details>
<summary>5.2. Lenet-5 - Parallel Version 1 (Im2Col) - Get unrolled image matrix by naive copy (Eigen)</s></summary>

    ../data/fashion-mnist/
    mnist train number: 60000
    mnist test number: 10000
    0-th batch, loss: 2.30436, time forwading avearage: 1092.89
    50-th batch, loss: 2.30307, time forwading avearage: 941.985
    100-th batch, loss: 2.30421, time forwading avearage: 945.737
    150-th batch, loss: 2.30438, time forwading avearage: 946.934
    200-th batch, loss: 2.30265, time forwading avearage: 948.548
    250-th batch, loss: 2.30305, time forwading avearage: 949.409
    300-th batch, loss: 2.30205, time forwading avearage: 949.609
    350-th batch, loss: 2.30245, time forwading avearage: 948.18
    400-th batch, loss: 2.3033, time forwading avearage: 947.395
    450-th batch, loss: 2.30154, time forwading avearage: 947.14

    1-th epoch, test acc: 0.1

    0-th batch, loss: 2.30235, time forwading avearage: 945.713
    50-th batch, loss: 2.3019, time forwading avearage: 945.826
    100-th batch, loss: 2.30197, time forwading avearage: 945.379
    150-th batch, loss: 2.30126, time forwading avearage: 944.503
    200-th batch, loss: 2.29878, time forwading avearage: 944.056
    250-th batch, loss: 2.29522, time forwading avearage: 942.984
    300-th batch, loss: 2.19485, time forwading avearage: 943.096
    350-th batch, loss: 1.52532, time forwading avearage: 941.864
    400-th batch, loss: 1.03688, time forwading avearage: 941.682
    450-th batch, loss: 0.98587, time forwading avearage: 940.09

    2-th epoch, test acc: 0.6093

    0-th batch, loss: 0.816876, time forwading avearage: 939.399
    50-th batch, loss: 0.873442, time forwading avearage: 938.74
    100-th batch, loss: 0.637464, time forwading avearage: 938.533
    150-th batch, loss: 0.89731, time forwading avearage: 938.154
    200-th batch, loss: 0.781668, time forwading avearage: 937.995
    250-th batch, loss: 0.702516, time forwading avearage: 937.752
    300-th batch, loss: 0.634439, time forwading avearage: 936.772
    350-th batch, loss: 0.765963, time forwading avearage: 936.633
    400-th batch, loss: 0.565993, time forwading avearage: 936.913
    450-th batch, loss: 0.692901, time forwading avearage: 936.621

    3-th epoch, test acc: 0.7663

    (The remaining is same as 4)
</details>

<details>
<summary>5.3. Lenet-5 - Parallel Version 1 (Im2Col) - Get unrolled image matrix avoid naive copy</summary>
    ../data/fashion-mnist/
    mnist train number: 60000
    mnist test number: 10000
    0-th batch, loss: 2.30436, time forwading avearage: 653.779
    50-th batch, loss: 2.30307, time forwading avearage: 696.657
    100-th batch, loss: 2.30421, time forwading avearage: 697.357
    150-th batch, loss: 2.30438, time forwading avearage: 696.52
    200-th batch, loss: 2.30265, time forwading avearage: 697.012
    250-th batch, loss: 2.30305, time forwading avearage: 697.045
    300-th batch, loss: 2.30205, time forwading avearage: 696.425
    350-th batch, loss: 2.30245, time forwading avearage: 695.164
    400-th batch, loss: 2.3033, time forwading avearage: 694.927
    450-th batch, loss: 2.30154, time forwading avearage: 696.307

    1-th epoch, test acc: 0.1

    0-th batch, loss: 2.30235, time forwading avearage: 696.599
    50-th batch, loss: 2.3019, time forwading avearage: 697.818
    100-th batch, loss: 2.30197, time forwading avearage: 697.937
    150-th batch, loss: 2.30126, time forwading avearage: 698.753
    200-th batch, loss: 2.29878, time forwading avearage: 698.786
    250-th batch, loss: 2.29522, time forwading avearage: 700.754
    300-th batch, loss: 2.19485, time forwading avearage: 700.852
    (The remaining is same as 5.2)

</details>

<details>
<summary>5.4. LeNet-5 - Parallel Version 1 (Im2Col) - Same as (C0), but get unrolled image results directly from dynamic memory to Matrix object</summary>

    ../data/fashion-mnist/
    mnist train number: 60000
    mnist test number: 10000
    0-th batch, loss: 2.30436, time forwading avearage: 503.376
    50-th batch, loss: 2.30307, time forwading avearage: 539.834
    100-th batch, loss: 2.30421, time forwading avearage: 539.134
    150-th batch, loss: 2.30438, time forwading avearage: 538.101
    200-th batch, loss: 2.30265, time forwading avearage: 539.293
    250-th batch, loss: 2.30305, time forwading avearage: 538.997
    300-th batch, loss: 2.30205, time forwading avearage: 538.233
    350-th batch, loss: 2.30245, time forwading avearage: 539.575
    400-th batch, loss: 2.3033, time forwading avearage: 540.215
    450-th batch, loss: 2.30154, time forwading avearage: 539.801

    1-th epoch, test acc: 0.1

    0-th batch, loss: 2.30235, time forwading avearage: 535.116
    50-th batch, loss: 2.3019, time forwading avearage: 535.02
    100-th batch, loss: 2.30197, time forwading avearage: 535.536
    150-th batch, loss: 2.30126, time forwading avearage: 538.216
    200-th batch, loss: 2.29878, time forwading avearage: 536.657
    250-th batch, loss: 2.29522, time forwading avearage: 536.651
    300-th batch, loss: 2.19485, time forwading avearage: 537.052
    350-th batch, loss: 1.52532, time forwading avearage: 536.631
    400-th batch, loss: 1.03688, time forwading avearage: 536.328
    450-th batch, loss: 0.98587, time forwading avearage: 538.418

    2-th epoch, test acc: 0.6093

    0-th batch, loss: 0.816876, time forwading avearage: 730.633
    50-th batch, loss: 0.873442, time forwading avearage: 533.947
    100-th batch, loss: 0.637464, time forwading avearage: 530.315
    150-th batch, loss: 0.89731, time forwading avearage: 527.891
    200-th batch, loss: 0.781668, time forwading avearage: 527.524
    250-th batch, loss: 0.702516, time forwading avearage: 527.48
    300-th batch, loss: 0.634439, time forwading avearage: 527.733
    350-th batch, loss: 0.765963, time forwading avearage: 527.928
    400-th batch, loss: 0.565993, time forwading avearage: 527.348
    450-th batch, loss: 0.692901, time forwading avearage: 527.813

    3-th epoch, test acc: 0.7663

    0-th batch, loss: 0.474801, time forwading avearage: 540.646
    50-th batch, loss: 0.562753, time forwading avearage: 533.911
    100-th batch, loss: 0.503141, time forwading avearage: 534.457
    150-th batch, loss: 0.501888, time forwading avearage: 532.106
    200-th batch, loss: 0.60367, time forwading avearage: 532.29
    250-th batch, loss: 0.631846, time forwading avearage: 532.792
    300-th batch, loss: 0.439007, time forwading avearage: 533.088
    350-th batch, loss: 0.581682, time forwading avearage: 532.95
    400-th batch, loss: 0.477746, time forwading avearage: 532.659
    450-th batch, loss: 0.382991, time forwading avearage: 531.859

    4-th epoch, test acc: 0.8123

    0-th batch, loss: 0.456361, time forwading avearage: 636.11
    50-th batch, loss: 0.394907, time forwading avearage: 536.47
    100-th batch, loss: 0.622893, time forwading avearage: 537.228
    150-th batch, loss: 0.633183, time forwading avearage: 533.514
    200-th batch, loss: 0.413122, time forwading avearage: 533.609
    250-th batch, loss: 0.415152, time forwading avearage: 533.462
    300-th batch, loss: 0.497327, time forwading avearage: 532.325
    350-th batch, loss: 0.581514, time forwading avearage: 532.735
    400-th batch, loss: 0.40746, time forwading avearage: 532.723
    450-th batch, loss: 0.429998, time forwading avearage: 531.433

    5-th epoch, test acc: 0.8297

<details>
